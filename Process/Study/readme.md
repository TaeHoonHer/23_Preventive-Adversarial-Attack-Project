## FGSM
- FGSM이란 'Fast Gradient Sign Method'의 약자로 딥러닝 모델을 공격하는 적대적 공격의 기술 중 하나이다
- adversarial attack 혹은 적대적 공격의 한 종류로서, 모델이 만들어낸 출력을 잘못 분류하게 만드는 방식으로 작동한다
- FGSM은 딥러닝 모델의 출력을 최대화하는 방향으로 입력 데이터가 작은 단위로 변형된다
  - 이렇게 하면 모델이 분류를 함에 있어 잘못된 결과를 내도록 할 수 있다. 본 과정은 기본적으로 모델의 gradient 정보를 이용하여 입력에 노이즈를 추가하는 것으로 생각을 할 수 있다
  - Gradient는 모델의 출력에 대한 입력의 민감도를 측정하는데 사용이 되며, 이를 통해 어떤 입력이 출력에 가장 큰 영향을 미치는지 파악할 수 있다

즉 정리를 하자면, FGSM은 gradient 정보를 사용하여 딥러닝 모델을 어지럽게(?) 만드는 방법을 의미한다

### 동작 원리
FGSM(Fast Gradient Sign Method)의 동작 원리를 이해하기 위해선, 딥러닝 모델의 학습 원리를 먼저 이해해야 한다
딥러닝 모델은 손실 함수(loss function)를 최소화하는 방향으로 가중치를 업데이트하는 과정을 거치며 학습을 진행을 하는데 이 때 그래디언트(gradient)는 손실 함수의 값을 줄이는 방향을 가리키는 지표로 작용한다

FGSM은 이 개념을 이용한 적대적 공격 방법이다.
모델이 예측하는 클래스에 대한 손실 함수를 증가시키는 방향으로 입력 데이터에 작은 변화를 주는 것이다. 
이 작은 변화는 사람의 눈에는 거의 눈치채지 못하지만, 딥러닝 모델은 이로 인해 잘못된 예측을 하게 된다

FGSM의 구체적인 동작 원리는 아래와 같다
- 먼저, 원본 이미지에 대한 모델의 그래디언트를 계산합니다. 그래디언트는 손실 함수에 대한 입력 이미지의 미분 값이며, 어떤 픽셀을 변경하면 손실이 얼마나 변하는지를 알려준다
- 계산된 그래디언트의 부호를 취한다
  - 이렇게 해서 얻은 값은 원본 이미지에 더해집니다. 이 때, 그래디언트의 크기 대신 부호를 사용하는 이유는 최대한 많은 픽셀에 작은 변화를 주어 원본 이미지와 가장 비슷하게 보이면서도 모델을 혼란스럽게 만들기 위함이다
- 이렇게 생성된 이미지를 적대적 예제(adversarial example)라고 부르며, 이 이미지는 원본 이미지와 매우 유사해 보이지만, 모델은 이를 잘못 분류하게 된다

FGSM은 상대적으로 간단하고 빠르게 적대적 예제를 생성할 수 있는 방법
